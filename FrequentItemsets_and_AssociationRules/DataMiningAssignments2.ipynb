{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery of Frequent Item-sets and Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequent item-set and associations rules are methods used in data mining in order to mining transaction data. A transaction data is a data where each rows represent the set of all items related to a buyer, user etc.  \n",
    "\n",
    "Given the follwing dataset :\n",
    "\n",
    "    [[1,2,3],\n",
    "    [1,4],\n",
    "    [4,5],\n",
    "    [1,2,4],\n",
    "    [1,2,6,4,3],\n",
    "    [2,6,3],\n",
    "    [2,3,6]]\n",
    "We need to find the all the frequent item-sets in data. And find association rules such as (1,2) -> 3, we means that there is a strong correlation that people who buy items 1 and 2 are more likely to buy item 3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support of an item-set : **\n",
    "The support of an item-set X is the number of times the item-set occurred in the transaction data : X.count\n",
    "\n",
    "\\begin{equation*}\n",
    "Support(X) = X.count\n",
    "\\end{equation*}\n",
    "\n",
    "**Support of a rule : **\n",
    "However the support of a rule $X \\rightarrow Y$ is the number of times X and Y occurred together by the length of the transaction data. \n",
    "\n",
    "\\begin{equation*}\n",
    "support(X \\rightarrow Y) = \\frac{(X \\cup Y).count}{N}\n",
    "\\end{equation*}\n",
    "where N = number of transactions. \n",
    "\n",
    "**Confidence of a rule :**\n",
    "\n",
    "The confidence of a rule $X \\rightarrow Y$ is the number of times X and Y occurred together by the support of X \n",
    "\\begin{equation*}\n",
    "confidence(X \\rightarrow Y) = \\frac{(X \\cup Y).count}{X.count}\n",
    "\\end{equation*}\n",
    "\n",
    "Confidence determines the predictability and the reliability of a rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools \n",
    "\n",
    "def loadTestDataSetA():\n",
    "    return [[1,2,3],[1,4],[4,5],[1,2,4],[1,2,6,4,3],[2,6,3],[2,3,6]]\n",
    "\n",
    "def findsubsets(s, n): \n",
    "    return [set(i) for i in itertools.combinations(s, n)] \n",
    "\n",
    "def init_pass(data) :\n",
    "    unique_items = []\n",
    "    \n",
    "    for transaction in data :\n",
    "        for item in transaction :\n",
    "            if {item} not in  unique_items :\n",
    "                unique_items.append({item})\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "def scanData(data, C1, minSupport = 0.4) :\n",
    "    \n",
    "    map = {}\n",
    "    unique_items = set()\n",
    "    for transaction in data :\n",
    "        for candidate in C1 :\n",
    "            if candidate.issubset(transaction) :\n",
    "                if len(candidate) < 2 :\n",
    "                    cand = tuple(candidate)[0]\n",
    "                    if cand in map :\n",
    "                        map[cand]  += 1\n",
    "                    else :\n",
    "                        map[cand] = 1      \n",
    "                else :\n",
    "                    \n",
    "                    if tuple(candidate) in map :\n",
    "                        map[tuple(candidate)]  += 1\n",
    "                    else :\n",
    "                        map[tuple(candidate)] = 1\n",
    "                       \n",
    "          \n",
    "    map_supp = {k: v / len(data)  for k, v in map.items() if v / len(data) >= minSupport}\n",
    "    unique_items = list(map_supp.keys())\n",
    "\n",
    "    return unique_items, map, map_supp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [1, 4], [4, 5], [1, 2, 4], [1, 2, 6, 4, 3], [2, 6, 3], [2, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "data = loadTestDataSetA()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the uniques item in transaction data, and calculate the frequent item-set for minsupp = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique items : \n",
      "[1, 2, 3, 4]\n",
      "Frequent item-set : \n",
      "{1: 0.5714285714285714, 2: 0.7142857142857143, 3: 0.5714285714285714, 4: 0.5714285714285714}\n"
     ]
    }
   ],
   "source": [
    "C1 = init_pass(data)\n",
    "minSupp = 0.5\n",
    "unique_items, maps, map_supp = scanData(data, C1, minSupp)\n",
    "print(\"Unique items : \")\n",
    "print(unique_items)\n",
    "print(\"Frequent item-set : \")\n",
    "print(map_supp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the support for item 1 is around 57% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequent item-set: **\n",
    "\n",
    "There is different level of frequent item-set :\n",
    "\n",
    "1. 1-level frequent item-set\n",
    "This is single item-sets that appears frequently in the transaction data with $support \\geq minsup$ .\n",
    "2. 2-level frequent item-set \n",
    "This is couple item-sets that appears frequently in the transaction data with $support \\geq minsup$ . \n",
    "2. N-level frequent item-set\n",
    "And so on...\n",
    "\n",
    "\n",
    "** Downward Closure Property : **\n",
    "\n",
    "*If an item-set has minimum support (or its support \\textbf{sup} is larger than \\textbf{minsup}), then its every non-empty subset also has minimum support.*\n",
    "\n",
    "\n",
    "The property states clearly that if we have a N-level frequent item-sets, then its every non-empty subset are frequent item-sets level N-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The apriori algorithm :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori algorithm is a bottom-up algorithm with begins by generating 1-level frequents item-sets and then used that to generate 2-level frequents item-sets and so on until there no new way to generate level N+1. \n",
    "\n",
    "The Apriori algorithm is implemented in two phases. First we generate the 1-level frequent item-sets, then we iterate over and over until we find all the levels item-sets. \n",
    "\n",
    "In general, We first generate (n)-level frequent item-set, and used that to generate candidates for (n+1)-level frequent item-set, by concatenating each element in the (n)-level frequent item-sets such as each subsets of each items the (n+1)-level frequent item-sets candidates are subset of the (n)-level frequent item-set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_generation(Fk, k) :\n",
    "    \n",
    "    length_k = len(Fk)\n",
    "    Ck = []\n",
    "    \n",
    "    element = Fk[0]\n",
    "    if isinstance(element, int) :\n",
    "        Fk = [ [x] for x in Fk]\n",
    "    else :\n",
    "        Fk = [ list(x) for x in Fk]\n",
    "        \n",
    "    for i in range(length_k) :\n",
    "        for j in range(i+1, length_k) :\n",
    "            L1 = list(Fk[i])\n",
    "            L2 = list(Fk[j])\n",
    "            L1.sort()\n",
    "            L2.sort()\n",
    "            if (L1[:-1] == L2[:-1]) and (L1[-1] != L2[-1]) :\n",
    "                candidate = set(L1) | set(L2)\n",
    "                subsets =  findsubsets(candidate, k-1)\n",
    "                ToAdd = True\n",
    "                for subset in subsets :\n",
    "                    if list(subset) not in Fk :\n",
    "                        ToAdd = False \n",
    "                if ToAdd :\n",
    "                    Ck.append(candidate)\n",
    "    return Ck\n",
    "\n",
    "def apriori_algorithm(dataset, minSupp) :\n",
    "    \n",
    "    C1 = init_pass(dataset)\n",
    "    unique_items, maps, map_supp = scanData(dataset, C1, minSupp)\n",
    "    #print(\"map : \", map)\n",
    "    A = []\n",
    "    B = []\n",
    "    C = [] \n",
    "    A.append(unique_items)\n",
    "    C.append(map_supp)\n",
    "    k = 2\n",
    "    while(unique_items) :\n",
    "        Ck = candidate_generation(unique_items, k)\n",
    "        unique_items, maper, map_supp = scanData(dataset, Ck, minSupp)\n",
    "        maps.update(maper)\n",
    "        A.append(unique_items)\n",
    "        C.append(map_supp)\n",
    "        k += 1\n",
    "    \n",
    "    return A, maps, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent unique_items: \n",
      "[[1, 2, 3, 4], [(2, 3)], []]\n",
      "map_supp : \n",
      "[{1: 0.5714285714285714, 2: 0.7142857142857143, 3: 0.5714285714285714, 4: 0.5714285714285714}, {(2, 3): 0.5714285714285714}, {}]\n"
     ]
    }
   ],
   "source": [
    "unique_items, maps,  map_supp = apriori_algorithm(data, minSupp) \n",
    "print(\"Frequent unique_items: \")\n",
    "print(unique_items)\n",
    "print(\"map_supp : \")\n",
    "print(map_supp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association rules generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the running of the apriori-alorithm we record the support each frequent itemset and the rules that will be generated will emane from the frequent itemsets. \n",
    "\n",
    "We generate association rules by looping over each n in $[1,..,N]$ such as :\n",
    "for each item-set $\\mathcal{T}$ in n-level frequent item-set ($n > 1$ and $n \\in [1,..,N]$ ), we generate each of its subset $\\beta$, then we calculate, the confidence of rule $(\\mathcal{T} - \\beta) \\rightarrow \\beta $ :\n",
    "\\begin{equation*}\n",
    "confidence((\\mathcal{T} - \\beta) \\rightarrow \\beta) = \\frac{((\\mathcal{T} - \\beta) \\rightarrow \\beta).count}{(\\mathcal{T} - \\beta).count}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(dataset,minSupp = 0.5, conf = 0.7 ) :\n",
    "    \n",
    "    uniques, map, map_support = apriori_algorithm(dataset, minSupp) \n",
    "    rules = []\n",
    "    for cnt, f in enumerate(uniques) :\n",
    "        if cnt >= 1 : \n",
    "            for itemset in f :\n",
    "                length_f = len(itemset)\n",
    "                for i in range(1,length_f) :\n",
    "                    subsets = findsubsets(itemset, i)\n",
    "                    for beta in subsets :\n",
    "                        f_b = set(itemset) - beta\n",
    "                        \n",
    "                        confidence = map[itemset] \n",
    "                        if len(f_b) <= 1 :\n",
    "                            confidence = confidence * 1.0 / map[list(f_b)[0]] \n",
    "                        else :\n",
    "                            confidence = confidence * 1.0 / map[tuple(f_b)] \n",
    "                            \n",
    "                        if confidence >= conf :\n",
    "                            rules.append((f_b, beta))\n",
    "       \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rules !!!! \n",
      "[({3}, {2}), ({6}, {2}), ({6}, {3}), ({3, 6}, {2}), ({2, 6}, {3}), ({6}, {2, 3})]\n"
     ]
    }
   ],
   "source": [
    "conf = 0.9\n",
    "minSupp = 0.4\n",
    "rules = generate_rules(data,minSupp,conf)\n",
    "print(\"The rules !!!! \")\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for sale transaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data is a sale transaction data of 100000 transactions in numerical form as figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData() :\n",
    "    datContent = [i.strip().split() for i in open(\"T10I4D100K.dat\").readlines()]\n",
    "    data = [  [int(y) for y in x] for x in datContent ]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 2 transactions : \n",
      "[[25, 52, 164, 240, 274, 328, 368, 448, 538, 561, 630, 687, 730, 775, 825, 834], [39, 120, 124, 205, 401, 581, 704, 814, 825, 834]]\n"
     ]
    }
   ],
   "source": [
    "data = loadData()\n",
    "print(\"The first 2 transactions : \")\n",
    "print(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
